{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62840f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats.mstats import winsorize\n",
    "import datetime\n",
    "import dateutil\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import statsmodels\n",
    "import xgboost\n",
    "import prophet\n",
    "import patsy\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5cbf5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fcc57242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval(num_folds=10):\n",
    "    file_path = 'Proj2_Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    #num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "        #print(test)\n",
    "        #print(f\"test.shape={test.shape}\")\n",
    "\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "        test_pred = test_pred.drop(columns=['IsHoliday'])\n",
    "        #print(test_pred)\n",
    "        #print(f\"test_pred.shape={test_pred.shape}\")\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "        #print(new_test)\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        #print(preds)\n",
    "        #print(actuals)\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    for value in wae:\n",
    "        print(f\"wae_by_fold={value:.3f}\")\n",
    "    print(f\"overall wae={sum(wae) / len(wae):.3f}\")\n",
    "    \n",
    "    # print(wae)\n",
    "    # print(np.mean(wae))    \n",
    "        \n",
    "    return wae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57cebf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "#    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7e667b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smooth_train(train):\n",
    "    smooth_dept_trains = []\n",
    "    departments = train['Dept'].unique()\n",
    "    stores = train['Store'].unique()\n",
    "    dates = train['Date'].unique()\n",
    "\n",
    "#     print(f\"departments.shape={departments.shape} \\n departments = \\n {departments}\")\n",
    "#     print(f\"stores.shape={stores.shape} \\n departments = \\n {stores}\")\n",
    "#     print(f\"dates.shape={dates.shape} \\n dates = \\n {dates}\")\n",
    "    \n",
    "    base_df = pd.DataFrame()\n",
    "    base_df['Date'] = dates\n",
    "    for store in stores:\n",
    "        base_df[store] = 0\n",
    "        \n",
    "#     print(f\"base_df.shape={base_df.shape}\")\n",
    "\n",
    "    for department in departments:\n",
    "        # Filter rows where Dept is equal to 1\n",
    "        filtered_train = train[train['Dept'] == department]\n",
    "        # Select only the columns 'Store', 'Date', and 'Weekly_Sales'\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        # Pivot table to spread 'Store' values into columns, with 'Weekly_Sales' as values\n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index()\n",
    "        \n",
    "#         print(f\"department={department}\")\n",
    "#         print(f\"train_dept_ts.shape={train_dept_ts.shape}\")\n",
    "        #print(f\"train_dept_ts = \\n {train_dept_ts}\")\n",
    "        train_dept_ts_tmp = base_df.copy()\n",
    "        train_dept_ts_tmp.iloc[:, 1:] = (base_df.iloc[:, 1:] + train_dept_ts.iloc[:, 1:]).fillna(0)\n",
    "        train_dept_ts = train_dept_ts_tmp\n",
    "#         print(f\"train_dept_ts_tmp.shape={train_dept_ts_tmp.shape}\")\n",
    "#         print(f\"train_dept_ts.shape={train_dept_ts.shape}\")\n",
    "        \n",
    "\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "\n",
    "        # Smooth department data\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "        store_means = np.mean(X_train, axis=0)\n",
    "        X_train = X_train - store_means\n",
    "        X_train = np.transpose(X_train)\n",
    "\n",
    "        U, D, V_t = np.linalg.svd(X_train, full_matrices=False)\n",
    "        D[8:] = 0\n",
    "        F_train = U @ np.diag(D) @ V_t\n",
    "\n",
    "        stores_list = train_dept_ts.columns[1:]\n",
    "        F_train = pd.DataFrame(np.transpose(F_train), columns=stores_list)\n",
    "        F_train = F_train.add(store_means, axis=1)\n",
    "        F_train[\"Date\"] = train_dept_ts[\"Date\"]\n",
    "\n",
    "        smooth_dept_train = pd.melt(F_train, id_vars=['Date'], value_vars = stores_list, \\\n",
    "                                    var_name='Store', value_name='Weekly_Sales')\n",
    "        smooth_dept_train[\"Dept\"] = department\n",
    "        smooth_dept_train[\"Store\"] = smooth_dept_train[\"Store\"].astype(np.int64)\n",
    "        smooth_dept_trains.append(smooth_dept_train)\n",
    "\n",
    "    smooth_train = pd.concat(smooth_dept_trains, ignore_index=True)\n",
    "    return smooth_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f813e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(train_file=\"train.csv\", test_file=\"test.csv\", pred_file=\"mypred.csv\", base_folder=\".\", make_post_prediction_adjustment=True):\n",
    "\n",
    "    # Reading train data\n",
    "    train_file_path = f\"{base_folder}/{train_file}\"\n",
    "    test_file_path = f\"{base_folder}/{test_file}\"\n",
    "    pred_file_path = f\"{base_folder}/{pred_file}\"\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    train = pd.read_csv(train_file_path)\n",
    "\n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    test = pd.read_csv(test_file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(\n",
    "        train_pairs, test_pairs, how=\"inner\", on=[\"Store\", \"Dept\"]\n",
    "    )\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr + np.power(Yr, 2) + Wk\",\n",
    "        data=train_split,\n",
    "        return_type=\"dataframe\",\n",
    "    )\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Yr ~ Store + Dept + Yr + np.power(Yr, 2) + Wk\", data=test_split, return_type=\"dataframe\"\n",
    "    )\n",
    "    X[\"Date\"] = test_split[\"Date\"]\n",
    "    test_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train[\"Weekly_Sales\"]\n",
    "        X_train = X_train.drop([\"Weekly_Sales\", \"Store\", \"Dept\"], axis=1)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[[\"Store\", \"Dept\", \"Date\"]]\n",
    "        X_test = X_test.drop([\"Store\", \"Dept\", \"Date\"], axis=1)\n",
    "\n",
    "        tmp_pred[\"Weekly_Pred\"] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    test_pred[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Post-prediction adjustment for fold 5\n",
    "    if make_post_prediction_adjustment:\n",
    "        dates = pd.to_datetime(test_pred[\"Date\"])\n",
    "        test_pred[\"Wk\"] = dates.dt.isocalendar().week\n",
    "\n",
    "        test_pred_51 = test_pred[test_pred[\"Wk\"] == 51]\n",
    "        test_pred_51[\"Shift\"] = test_pred_51[\"Weekly_Pred\"] / 9\n",
    "        test_pred_52 = test_pred[test_pred[\"Wk\"] == 52]\n",
    "\n",
    "        test_pred_52 = test_pred_52.merge(\n",
    "            test_pred_51[[\"Store\", \"Dept\", \"Shift\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        test_pred_51 = test_pred_51.merge(\n",
    "            test_pred_52[[\"Store\", \"Dept\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\", indicator=True\n",
    "        )\n",
    "        test_pred_51[test_pred_51[\"_merge\"] == \"left_only\"][\"Shift\"] = 0\n",
    "\n",
    "        test_pred_52[\"Date\"] = \"2011-12-30\"\n",
    "        test_pred_52[\"Shift\"].fillna(0, inplace=True)\n",
    "        test_pred_52[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "        # test_pred_51[\"Weekly_Pred\"] = test_pred_51[\"Weekly_Pred\"] - test_pred_51[\"Shift\"]\n",
    "        test_pred_52[\"Weekly_Pred\"] = test_pred_52[\"Weekly_Pred\"] + test_pred_52[\"Shift\"]\n",
    "\n",
    "        test_pred_51.drop(\"Shift\", inplace=True, axis=1)\n",
    "        test_pred_52.drop(\"Shift\", inplace=True, axis=1)\n",
    "\n",
    "        test_pred = test_pred[(test_pred[\"Wk\"] != 51) & (test_pred[\"Wk\"] != 52)]\n",
    "        test_pred = pd.concat([test_pred, test_pred_51, test_pred_52], ignore_index=True)\n",
    "        test_pred.drop(columns=[\"Wk\"], inplace=True)\n",
    "\n",
    "    # Save the output to CSV\n",
    "    test_pred[\"Store\"] = test_pred[\"Store\"].astype(np.int64)\n",
    "    test_pred[\"Dept\"] = test_pred[\"Dept\"].astype(np.int64)\n",
    "    \n",
    "    test_pred_final = test.merge(test_pred, on=[\"Store\", \"Dept\", \"Date\"], how=\"left\")\n",
    "    test_pred_final[\"Weekly_Pred\"] = test_pred_final[\"Weekly_Pred\"].fillna(0)\n",
    "    test_pred_final = test_pred_final.loc[:, [\"Store\", \"Dept\", \"Date\", \"IsHoliday\", \"Weekly_Pred\"]]\n",
    "    test_pred_final.to_csv(pred_file_path, index=False)\n",
    "    print(pred_file_path)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a37b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▊                                                                                                          | 1/10 [00:09<01:24,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n",
      "wae_by_fold=1941.580\n",
      "overall wae=1941.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▌                                                                                              | 2/10 [00:19<01:17,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_2/mypred.csv\n",
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "overall wae=1652.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [00:28<01:06,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_3/mypred.csv\n",
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "overall wae=1562.551\n",
      "Proj2_Data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [00:38<00:58,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "overall wae=1553.773\n",
      "Proj2_Data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████                                                           | 5/10 [01:04<01:17, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "overall wae=1685.312\n",
      "Proj2_Data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 6/10 [01:23<01:06, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "wae_by_fold=1638.093\n",
      "overall wae=1677.442\n",
      "Proj2_Data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████▌                                   | 7/10 [01:40<00:50, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "wae_by_fold=1638.093\n",
      "wae_by_fold=1615.087\n",
      "overall wae=1668.534\n",
      "Proj2_Data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▍                       | 8/10 [02:00<00:35, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "wae_by_fold=1638.093\n",
      "wae_by_fold=1615.087\n",
      "wae_by_fold=1356.711\n",
      "overall wae=1629.557\n",
      "Proj2_Data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9/10 [02:31<00:21, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "wae_by_fold=1638.093\n",
      "wae_by_fold=1615.087\n",
      "wae_by_fold=1356.711\n",
      "wae_by_fold=1337.922\n",
      "overall wae=1597.153\n",
      "Proj2_Data/fold_10/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:45<00:00, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.580\n",
      "wae_by_fold=1363.542\n",
      "wae_by_fold=1382.531\n",
      "wae_by_fold=1527.440\n",
      "wae_by_fold=2211.467\n",
      "wae_by_fold=1638.093\n",
      "wae_by_fold=1615.087\n",
      "wae_by_fold=1356.711\n",
      "wae_by_fold=1337.922\n",
      "wae_by_fold=1334.634\n",
      "overall wae=1570.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "\n",
    "for j in tqdm(range(1, num_folds + 1)):\n",
    "\n",
    "    base_folder = f\"Proj2_Data/fold_{j}\"\n",
    "    # Reading train data\n",
    "    train_file = \"train.csv\"\n",
    "    test_file = \"test.csv\"\n",
    "    \n",
    "    process_model(train_file=train_file, test_file=test_file, base_folder=base_folder, make_post_prediction_adjustment=True)\n",
    "    myeval(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c83d2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1941.581\n",
      "wae_by_fold=1363.493\n",
      "wae_by_fold=1382.461\n",
      "wae_by_fold=1527.275\n",
      "wae_by_fold=2210.984\n",
      "wae_by_fold=1635.292\n",
      "wae_by_fold=1613.891\n",
      "wae_by_fold=1355.014\n",
      "wae_by_fold=1336.916\n",
      "wae_by_fold=1334.010\n",
      "overall wae=1570.092\n"
     ]
    }
   ],
   "source": [
    "wae = myeval(num_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b278b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
