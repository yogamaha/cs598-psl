{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688164c9",
   "metadata": {},
   "source": [
    "### Libraries Allowed\n",
    "pandas, scipy, numpy\n",
    "sklearn\n",
    "datetime\n",
    "dateutil\n",
    "prophet\n",
    "patsy (for model matrix), statsmodels\n",
    "xgboost\n",
    "warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62840f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats.mstats import winsorize\n",
    "import datetime\n",
    "import dateutil\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import statsmodels\n",
    "import xgboost\n",
    "import prophet\n",
    "import patsy\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac515c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf284b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import random\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "f7742d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A    B\n",
       "0  1  2.0\n",
       "1  2  4.0\n",
       "2  3  0.0"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [3, 4, 5]})\n",
    "#df.columns[0:]\n",
    "\n",
    "df2 = pd.DataFrame({'A': [1, 2], 'C': [3, 4]})\n",
    "\n",
    "df3 = df1\n",
    "df3.iloc[:, 1:] = (df1.iloc[:, :] + df2.iloc[:, :]).fillna(0)\n",
    "\n",
    "df3\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "df1a7da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[618], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m winsorize\n\u001b[0;32m----> 3\u001b[0m winsorize([\u001b[38;5;241m92\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m58\u001b[39m, \u001b[38;5;241m1053\u001b[39m, \u001b[38;5;241m91\u001b[39m, \u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m78\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m86\u001b[39m, \u001b[38;5;241m85\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m89\u001b[39m, \u001b[38;5;241m89\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m41\u001b[39m], limits\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.05\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs598-psl-env/lib/python3.11/site-packages/scipy/stats/_mstats_basic.py:2524\u001b[0m, in \u001b[0;36mwinsorize\u001b[0;34m(a, limits, inclusive, inplace, axis, nan_policy)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             a[idx[upidx:]] \u001b[38;5;241m=\u001b[39m a[idx[upidx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m   2522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m-> 2524\u001b[0m contains_nan, nan_policy \u001b[38;5;241m=\u001b[39m _contains_nan(a, nan_policy)\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;66;03m# We are going to modify a: better make a copy\u001b[39;00m\n\u001b[1;32m   2526\u001b[0m a \u001b[38;5;241m=\u001b[39m ma\u001b[38;5;241m.\u001b[39marray(a, copy\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlogical_not(inplace))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs598-psl-env/lib/python3.11/site-packages/scipy/_lib/_util.py:639\u001b[0m, in \u001b[0;36m_contains_nan\u001b[0;34m(a, nan_policy, use_summation, policies)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m policies:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan_policy must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    637\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m policies))\n\u001b[0;32m--> 639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(a\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# The summation method avoids creating a (potentially huge) array.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_summation:\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf473568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_file, test_file, test_with_label_file):\n",
    "    # Fill in later\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a77b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 1\n",
    "for i in tqdm(range(1, fold_count + 1)):\n",
    "    train_file = f\"Proj2_Data/fold_{i}/train.csv\"\n",
    "    test_file = f\"Proj2_Data/fold_{i}/test.csv\"\n",
    "    test_with_label_file = f\"Proj2_Data/test_with_label.csv\"\n",
    "    process_data(train_file, test_file, test_with_label_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_file={train_file}\")\n",
    "train_df = pd.read_csv(train_file)\n",
    "train_df.head()\n",
    "\n",
    "plt.\n",
    "\n",
    "print(f\"test_file={test_file}\")\n",
    "test_df = pd.read_csv(test_file)\n",
    "test_df.head()\n",
    "\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0258293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"Proj2_Data/test_with_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
    "ax.plot([1, 2, 3, 4], [1, 4, 2, 3])  # Plot some data on the axes.\n",
    "\n",
    "fig, axs = plt.subplots(2,2)  # Create a figure containing a single axes.\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i,j].plot([1, 2, 3, 4], [1, 4, 2, 3])  # Plot some data on the axes.\n",
    "        print()\n",
    "\n",
    "plt.subplot_mosaic([['left_top', 'right'],\n",
    "                    ['left', 'right']])\n",
    "\n",
    "#plt.plot([1, 2, 3, 4], [1, 4, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([1, 2, 3, 4], [1, 4, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Date'] = pd.to_datetime(train_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df_tmp = train_df[1:60]\n",
    "plt.plot(train_df_tmp['Date'], train_df_tmp['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(train_df['Date']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28876830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0:5, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new = train_df.set_index(['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d85e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new.iloc[56:112].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb78176",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_df['Date'], train_df['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cec32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new.iloc[56:112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "57747cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval(num_folds=10):\n",
    "    file_path = 'Proj2_Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    #num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    for value in wae:\n",
    "        print(f\"wae_by_fold={value:.3f}\")\n",
    "    print(f\"overall wae={sum(wae) / len(wae):.3f}\")\n",
    "    \n",
    "    # print(wae)\n",
    "    # print(np.mean(wae))    \n",
    "        \n",
    "    return wae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfc780",
   "metadata": {},
   "source": [
    "### Approach#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 1\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "#i = 1\n",
    "# Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{i}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{i}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    most_recent_date = train['Date'].max()\n",
    "\n",
    "    tmp_train = train[train['Date'] == most_recent_date].copy()\n",
    "\n",
    "    # Filter and select necessary columns\n",
    "    tmp_train.rename(columns={'Weekly_Sales': 'Weekly_Pred'}, inplace=True)\n",
    "    tmp_train = tmp_train.drop(columns=['Date', 'IsHoliday'])\n",
    "\n",
    "\n",
    "    # Left join with the test data\n",
    "    test_pred = test.merge(tmp_train, on=['Dept', 'Store'], how='left')\n",
    "\n",
    "    #test_pred['Weekly_Pred'] = 0\n",
    "\n",
    "    # Fill NaN values with 0 for the Weekly_Pred column\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    test_pred = test_pred.drop(columns=[\"IsHoliday\"])\n",
    "\n",
    "    # Write the output to CSV\n",
    "    file_path = f'Proj2_Data/fold_{i}/mypred.csv'\n",
    "    test_pred.to_csv(file_path, index=False)\n",
    "\n",
    "#test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")\n",
    "\n",
    "print(wae)\n",
    "print(np.mean(wae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d08f03",
   "metadata": {},
   "source": [
    "### Approach - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    # Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{i}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{i}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # Define start and end dates based on test data\n",
    "    start_last_year = pd.to_datetime(test['Date'].min()) - timedelta(days=375)\n",
    "    end_last_year = pd.to_datetime(test['Date'].max()) - timedelta(days=350)\n",
    "\n",
    "    # Filter train data based on the defined dates and compute 'Wk' column\n",
    "    tmp_train = train[(train['Date'] > str(start_last_year)) \n",
    "                      & (train['Date'] < str(end_last_year))].copy()\n",
    "    tmp_train['Date'] = pd.to_datetime(tmp_train['Date'])  \n",
    "    tmp_train['Wk'] = tmp_train['Date'].dt.isocalendar().week\n",
    "    tmp_train.rename(columns={'Weekly_Sales': 'Weekly_Pred'}, inplace=True)\n",
    "    tmp_train.drop(columns=['Date', 'IsHoliday'], inplace=True)\n",
    "\n",
    "    # Compute 'Wk' column for test data\n",
    "    test['Date'] = pd.to_datetime(test['Date'])\n",
    "    test['Wk'] = test['Date'].dt.isocalendar().week\n",
    "\n",
    "    # Left join with the tmp_train data\n",
    "    test_pred = test.merge(tmp_train, on=['Dept', 'Store', 'Wk'], how='left').drop(columns=['Wk'])\n",
    "\n",
    "    # Fill NaN values with 0 for the Weekly_Pred column\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    test_pred = test_pred.drop(columns=[\"IsHoliday\"])\n",
    "\n",
    "    # Save the output to CSV\n",
    "    file_path = f'Proj2_Data/fold_{i}/mypred.csv'\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c786edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")\n",
    "\n",
    "print(wae)\n",
    "print(np.mean(wae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e7786",
   "metadata": {},
   "source": [
    "### Approach - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "de343388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "#    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "fb33be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n",
      "CPU times: user 2min 15s, sys: 1min 24s, total: 3min 39s\n",
      "Wall time: 26.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The code is for fold number =2\n",
    "\n",
    "foldnum = 2\n",
    "\n",
    "#for foldnum in tqdm(range(1, 11)):\n",
    "for foldnum in tqdm([1]):\n",
    "    # Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "\n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/mypred.csv'\n",
    "    print(file_path)\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "73106859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=2049.347\n",
      "overall wae=2049.347\n",
      "CPU times: user 341 ms, sys: 377 ms, total: 717 ms\n",
      "Wall time: 93.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wae = myeval(num_folds=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa288a",
   "metadata": {},
   "source": [
    "### Approach - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "c34499ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "        tmp = pd.to_datetime(data[\"Date\"])\n",
    "        data[\"Wk\"] = tmp.dt.isocalendar().week\n",
    "        data[\"Yr\"] = tmp.dt.year\n",
    "        data[\"Wk\"] = pd.Categorical(data[\"Wk\"], categories=[i for i in range(1, 53)])  # 52 weeks\n",
    "        # data[\"IsHoliday\"] = data[\"IsHoliday\"].apply(int)\n",
    "        return data\n",
    "\n",
    "def pca_smooth_train_v1(train):\n",
    "    \"\"\"\n",
    "    https://campuswire.com/c/G06C55090/feed/737\n",
    "\n",
    "    Your description sounds right: Smooth train one dept at a time, combine each smoothed store data to create the whole smoothed train.\n",
    "    Make sure you use this smooth dataframe in subsequent steps. Otherwise, the errors will be very high.\n",
    "\n",
    "    Also, make sure you subtract the mean correctly. There are two ways:  get the mean by store and dept or just by store as outlined in\n",
    "    the Professor's post. I ended up using (Store, Dept). You can actually run the whole process without subtracting the mean and once all\n",
    "    the other logic is working correctly  you can add this process to see how it impacts the results. Divide and conquer. In my case,\n",
    "    I got 1,583 without subtracting the mean before SVD, which is slightly better compared to 1,589.\n",
    "\n",
    "    Regarding SVD, I used k=8 except when there are fewer components. In that case,  I am using S.shape.\n",
    "\n",
    "    Last, when multiplying the matrices I am using: U @ S @ Vt without forcing (S @ Vt) first. It seems some people got a different result.\n",
    "    I am using Windows and Python 3.11 and that differ in other platforms.\n",
    "    \"\"\"\n",
    "\n",
    "    smooth_dept_trains = []\n",
    "    departments = train['Dept'].unique()\n",
    "\n",
    "#     print(f\"train=\\n {train}\")\n",
    "#     print(f\"departments=\\n {departments} {len(departments)}\")\n",
    "#     print(f\"train['Store'].unique()=\\n {train['Store'].unique()} {len(train['Store'].unique())}\")\n",
    "    \n",
    "    for department in departments:\n",
    "        # Filter rows where Dept is equal to 1\n",
    "        filtered_train = train[train['Dept'] == department]\n",
    "        # Select only the columns 'Store', 'Date', and 'Weekly_Sales'\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        # Pivot table to spread 'Store' values into columns, with 'Weekly_Sales' as values\n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index()\n",
    "        # Smooth department data\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "        #print(f\"train_dept_ts = \\n  {train_dept_ts}\")\n",
    "        \n",
    "#         print(f\"filtered_train=\\n {filtered_train}\")\n",
    "#         print(f\"train_dept_ts=\\n {train_dept_ts}\")\n",
    "        \n",
    "\n",
    "        # store_means = X_train.mean(axis=1).to_list()\n",
    "        # X_train = X_train.sub(store_means, axis=0)\n",
    "\n",
    "        X_train = X_train.to_numpy()\n",
    "        store_means = np.nanmean(X_train, axis=0)\n",
    "        inds = np.where(np.isnan(X_train))\n",
    "        X_train[inds] = np.take(store_means, inds[1])\n",
    "\n",
    "        try:\n",
    "            U, D, V_t = np.linalg.svd(X_train, full_matrices=False)\n",
    "            D[8:] = 0\n",
    "            F_train = U @ np.diag(D) @ V_t\n",
    "            F_train = pd.DataFrame(F_train, columns=[str(x) for x in range(1, F_train.shape[1]+1)])\n",
    "\n",
    "            # F_train = F_train.add(store_means, axis=0)\n",
    "\n",
    "            F_train[\"Date\"] = train_dept_ts[\"Date\"]\n",
    "            smooth_dept_train = pd.melt(F_train, id_vars=['Date'], var_name=['Store'], value_name='Weekly_Sales')\n",
    "            smooth_dept_train[\"Dept\"] = department\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            smooth_dept_train = selected_columns\n",
    "\n",
    "        smooth_dept_train[\"Store\"] = smooth_dept_train[\"Store\"].astype(np.int64)\n",
    "        smooth_dept_trains.append(smooth_dept_train)\n",
    "\n",
    "    smooth_train = pd.concat(smooth_dept_trains, ignore_index=True)\n",
    "    return smooth_train\n",
    "\n",
    "def pca_smooth_train_v2(train):\n",
    "    smooth_dept_trains = []\n",
    "    departments = train['Dept'].unique()\n",
    "\n",
    "    for department in departments:\n",
    "        # Filter rows where Dept is equal to 1\n",
    "        filtered_train = train[train['Dept'] == department]\n",
    "        # Select only the columns 'Store', 'Date', and 'Weekly_Sales'\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        # Pivot table to spread 'Store' values into columns, with 'Weekly_Sales' as values\n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index()\n",
    "\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "\n",
    "        # Smooth department data\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "        store_means = np.mean(X_train, axis=0)\n",
    "        X_train = X_train - store_means\n",
    "        X_train = np.transpose(X_train)\n",
    "\n",
    "        U, D, V_t = np.linalg.svd(X_train, full_matrices=False)\n",
    "        D[8:] = 0\n",
    "        F_train = U @ np.diag(D) @ V_t\n",
    "\n",
    "        stores_list = train_dept_ts.columns[1:]\n",
    "        F_train = pd.DataFrame(np.transpose(F_train), columns=stores_list)\n",
    "        F_train = F_train.add(store_means, axis=1)\n",
    "        F_train[\"Date\"] = train_dept_ts[\"Date\"]\n",
    "\n",
    "        smooth_dept_train = pd.melt(F_train, id_vars=['Date'], value_vars = stores_list, \\\n",
    "                                    var_name='Store', value_name='Weekly_Sales')\n",
    "        smooth_dept_train[\"Dept\"] = department\n",
    "        smooth_dept_train[\"Store\"] = smooth_dept_train[\"Store\"].astype(np.int64)\n",
    "        smooth_dept_trains.append(smooth_dept_train)\n",
    "\n",
    "    smooth_train = pd.concat(smooth_dept_trains, ignore_index=True)\n",
    "    return smooth_train\n",
    "\n",
    "def pca_smooth_train(train):\n",
    "    return pca_smooth_train_v2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "45e29edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▊                                                                                                          | 1/10 [00:25<03:52, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▌                                                                                              | 2/10 [00:54<03:39, 27.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_2/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [01:23<03:16, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_3/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [02:01<03:12, 32.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████                                                           | 5/10 [02:38<02:49, 33.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 6/10 [03:30<02:40, 40.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████▌                                   | 7/10 [04:19<02:09, 43.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▍                       | 8/10 [05:21<01:37, 48.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9/10 [06:03<00:46, 46.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [06:40<00:00, 40.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_10/mypred.csv\n",
      "CPU times: user 33min 23s, sys: 20min 16s, total: 53min 40s\n",
      "Wall time: 6min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The code is for fold number =2\n",
    "\n",
    "foldnum = 2\n",
    "\n",
    "for foldnum in tqdm(range(1, 11)):\n",
    "#for foldnum in tqdm([1]):\n",
    "    # Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "    \n",
    "#     print(f\"train=\\n {train}\")\n",
    "    \n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "    #unique_pairs = pd.merge(train_pairs, test_pairs, how = 'outer', on =['Store', 'Dept'])\n",
    "#     print(f\"train_pairs={train_pairs}\")\n",
    "#     print(f\"test_pairs={test_pairs}\")\n",
    "#     print(f\"unique_pairs={unique_pairs}\")\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "#     print(f\"train_split=\\n{train_split}\")\n",
    "    train_split = preprocess(train_split)\n",
    "#     print(f\"train_split=\\n{train_split}\")\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "#     print(f\"y=\\n{y}\")\n",
    "#     print(f\"X=\\n{X}\")\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "#     print(f\"keys=\\n{keys}\")\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "\n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/mypred.csv'\n",
    "    print(file_path)\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "b81a034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1945.900\n",
      "wae_by_fold=1364.163\n",
      "wae_by_fold=1384.109\n",
      "wae_by_fold=1528.781\n",
      "wae_by_fold=2320.452\n",
      "wae_by_fold=1638.096\n",
      "wae_by_fold=1684.565\n",
      "wae_by_fold=1400.063\n",
      "wae_by_fold=1418.869\n",
      "wae_by_fold=1426.822\n",
      "overall wae=1611.182\n",
      "CPU times: user 798 ms, sys: 442 ms, total: 1.24 s\n",
      "Wall time: 458 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wae = myeval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "4b00702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▊                                                                                                          | 1/10 [00:29<04:29, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▌                                                                                              | 2/10 [00:59<03:59, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_2/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [01:33<03:42, 31.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_3/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [02:09<03:20, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████                                                           | 5/10 [02:48<02:56, 35.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 6/10 [03:32<02:33, 38.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████▌                                   | 7/10 [04:17<02:01, 40.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▍                       | 8/10 [05:16<01:32, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9/10 [06:11<00:49, 49.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [07:05<00:00, 42.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_10/mypred.csv\n",
      "CPU times: user 37min 51s, sys: 21min 10s, total: 59min 1s\n",
      "Wall time: 7min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The code is for fold number =2\n",
    "\n",
    "foldnum = 2\n",
    "\n",
    "for foldnum in tqdm(range(1, 11)):\n",
    "#for foldnum in tqdm([1]):\n",
    "    # Reading train data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/train.csv'\n",
    "    train = pd.read_csv(file_path)\n",
    "    \n",
    "#     print(f\"train=\\n {train}\")\n",
    "    \n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/test.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "    #unique_pairs = pd.merge(train_pairs, test_pairs, how = 'outer', on =['Store', 'Dept'])\n",
    "#     print(f\"train_pairs={train_pairs}\")\n",
    "#     print(f\"test_pairs={test_pairs}\")\n",
    "#     print(f\"unique_pairs={unique_pairs}\")\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "#     print(f\"train_split=\\n{train_split}\")\n",
    "    train_split = preprocess(train_split)\n",
    "#     print(f\"train_split=\\n{train_split}\")\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + np.power(Yr, 2)+ Wk', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "#     print(f\"y=\\n{y}\")\n",
    "#     print(f\"X=\\n{X}\")\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + np.power(Yr, 2) + Wk', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "#     print(f\"keys=\\n{keys}\")\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "\n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "\n",
    "    file_path = f'Proj2_Data/fold_{foldnum}/mypred.csv'\n",
    "    print(file_path)\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d8537f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1945.900\n",
      "wae_by_fold=1364.163\n",
      "wae_by_fold=1384.109\n",
      "wae_by_fold=1528.781\n",
      "wae_by_fold=2320.452\n",
      "wae_by_fold=1638.096\n",
      "wae_by_fold=1614.847\n",
      "wae_by_fold=1355.272\n",
      "wae_by_fold=1337.454\n",
      "wae_by_fold=1334.495\n",
      "overall wae=1582.357\n",
      "CPU times: user 482 ms, sys: 241 ms, total: 723 ms\n",
      "Wall time: 631 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wae = myeval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "e7492917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- fold 1, 1945.895, 8.539409875869751 seconds ---\n",
      "--- fold 2, 1364.090, 10.020219087600708 seconds ---\n",
      "--- fold 3, 1384.009, 10.18326711654663 seconds ---\n",
      "--- fold 4, 1528.647, 11.091276168823242 seconds ---\n",
      "--- fold 5, 2220.198, 13.665432691574097 seconds ---\n",
      "--- fold 6, 1636.200, 20.71449303627014 seconds ---\n",
      "--- fold 7, 1614.822, 13.318205833435059 seconds ---\n",
      "--- fold 8, 1355.266, 13.36004090309143 seconds ---\n",
      "--- fold 9, 1337.461, 12.044707775115967 seconds ---\n",
      "--- fold 10, 1334.495, 12.105077028274536 seconds ---\n",
      "CPU times: user 11min 12s, sys: 5min 13s, total: 16min 26s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "file_path = \"Proj2_Data/test_with_label.csv\"\n",
    "test_with_label = pd.read_csv(file_path)\n",
    "\n",
    "for j in range(1, num_folds + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Reading train data\n",
    "    file_path = f\"Proj2_Data/fold_{j}/train.csv\"\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f\"Proj2_Data/fold_{j}/test.csv\"\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(\n",
    "        train_pairs, test_pairs, how=\"inner\", on=[\"Store\", \"Dept\"]\n",
    "    )\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr + np.power(Yr, 2)  + Wk\",\n",
    "        data=train_split,\n",
    "        return_type=\"dataframe\",\n",
    "    )\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Yr ~ Store + Dept + Yr + np.power(Yr, 2) + Wk\", data=test_split, return_type=\"dataframe\"\n",
    "    )\n",
    "    X[\"Date\"] = test_split[\"Date\"]\n",
    "    test_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train[\"Weekly_Sales\"]\n",
    "        X_train = X_train.drop([\"Weekly_Sales\", \"Store\", \"Dept\"], axis=1)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[[\"Store\", \"Dept\", \"Date\"]]\n",
    "        X_test = X_test.drop([\"Store\", \"Dept\", \"Date\"], axis=1)\n",
    "\n",
    "        tmp_pred[\"Weekly_Pred\"] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Post-prediction adjustment for fold 5\n",
    "    if j == 5:\n",
    "        dates = pd.to_datetime(test_pred[\"Date\"])\n",
    "        test_pred[\"Wk\"] = dates.dt.isocalendar().week\n",
    "\n",
    "        test_pred_51 = test_pred[test_pred[\"Wk\"] == 51]\n",
    "        test_pred_51[\"Shift\"] = test_pred_51[\"Weekly_Pred\"] / 9\n",
    "        test_pred_52 = test_pred[test_pred[\"Wk\"] == 52]\n",
    "\n",
    "        test_pred_52 = test_pred_52.merge(\n",
    "            test_pred_51[[\"Store\", \"Dept\", \"Shift\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        test_pred_51 = test_pred_51.merge(\n",
    "            test_pred_52[[\"Store\", \"Dept\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\", indicator=True\n",
    "        )\n",
    "        test_pred_51[test_pred_51[\"_merge\"] == \"left_only\"][\"Shift\"] = 0\n",
    "\n",
    "        test_pred_52[\"Date\"] = \"2011-12-30\"\n",
    "        test_pred_52[\"Shift\"].fillna(0, inplace=True)\n",
    "        test_pred_52[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "        # test_pred_51[\"Weekly_Pred\"] = test_pred_51[\"Weekly_Pred\"] - test_pred_51[\"Shift\"]\n",
    "        test_pred_52[\"Weekly_Pred\"] = test_pred_52[\"Weekly_Pred\"] + test_pred_52[\"Shift\"]\n",
    "\n",
    "        test_pred_51.drop(\"Shift\", inplace=True, axis=1)\n",
    "        test_pred_52.drop(\"Shift\", inplace=True, axis=1)\n",
    "\n",
    "        test_pred = test_pred[(test_pred[\"Wk\"] != 51) & (test_pred[\"Wk\"] != 52)]\n",
    "        test_pred = pd.concat([test_pred, test_pred_51, test_pred_52], ignore_index=True)\n",
    "        test_pred.drop(columns=[\"Wk\"], inplace=True)\n",
    "\n",
    "    # Save the output to CSV\n",
    "    file_path = f\"Proj2_Data/fold_{j}/mypred.csv\"\n",
    "    test_pred.to_csv(file_path, index=False)\n",
    "\n",
    "    print(\n",
    "        \"--- fold {fold}, {wae_fold}, {elapsed} seconds ---\".format(\n",
    "            fold=j,\n",
    "            wae_fold=f\"{myeval_fold(j, test_with_label):.3f}\",\n",
    "            elapsed=time.time() - start_time\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "6da661fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1945.895\n",
      "wae_by_fold=1364.090\n",
      "wae_by_fold=1384.009\n",
      "wae_by_fold=1528.647\n",
      "wae_by_fold=2220.198\n",
      "wae_by_fold=1636.200\n",
      "wae_by_fold=1614.822\n",
      "wae_by_fold=1355.266\n",
      "wae_by_fold=1337.461\n",
      "wae_by_fold=1334.495\n",
      "overall wae=1572.108\n",
      "CPU times: user 680 ms, sys: 428 ms, total: 1.11 s\n",
      "Wall time: 466 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wae = myeval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20260ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
