{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62840f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats.mstats import winsorize\n",
    "import datetime\n",
    "import dateutil\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import statsmodels\n",
    "# import xgboost\n",
    "# import prophet\n",
    "import patsy\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbf5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc57242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval(num_folds=10):\n",
    "    file_path = 'Proj2_Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    #num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "        #print(test)\n",
    "        #print(f\"test.shape={test.shape}\")\n",
    "\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "        test_pred = test_pred.drop(columns=['IsHoliday'])\n",
    "        #print(test_pred)\n",
    "        #print(f\"test_pred.shape={test_pred.shape}\")\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "        #print(new_test)\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        #print(preds)\n",
    "        #print(actuals)\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    i = 1\n",
    "    for value in wae:\n",
    "        print(f\"wae_by_fold_{i}={value:.3f}\")\n",
    "        i += 1\n",
    "    print(f\"overall wae={sum(wae) / len(wae):.3f}\")\n",
    "    \n",
    "    # print(wae)\n",
    "    # print(np.mean(wae))    \n",
    "        \n",
    "    return wae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cebf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "#    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e667b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smooth_train(train):\n",
    "    smooth_dept_trains = []\n",
    "    departments = train['Dept'].unique()\n",
    "\n",
    "    for department in departments:\n",
    "        # Filter rows where Dept is equal to 1\n",
    "        filtered_train = train[train['Dept'] == department]\n",
    "        # Select only the columns 'Store', 'Date', and 'Weekly_Sales'\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        # Pivot table to spread 'Store' values into columns, with 'Weekly_Sales' as values\n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index()\n",
    "\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "\n",
    "        # Smooth department data\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "        store_means = np.mean(X_train, axis=0)\n",
    "        X_train = X_train - store_means\n",
    "        X_train = np.transpose(X_train)\n",
    "\n",
    "        U, D, V_t = np.linalg.svd(X_train, full_matrices=False)\n",
    "        D[8:] = 0\n",
    "        F_train = U @ np.diag(D) @ V_t\n",
    "\n",
    "        stores_list = train_dept_ts.columns[1:]\n",
    "        F_train = pd.DataFrame(np.transpose(F_train), columns=stores_list)\n",
    "        F_train = F_train.add(store_means, axis=1)\n",
    "        F_train[\"Date\"] = train_dept_ts[\"Date\"]\n",
    "\n",
    "        smooth_dept_train = pd.melt(F_train, id_vars=['Date'], value_vars = stores_list, \\\n",
    "                                    var_name='Store', value_name='Weekly_Sales')\n",
    "        smooth_dept_train[\"Dept\"] = department\n",
    "        smooth_dept_train[\"Store\"] = smooth_dept_train[\"Store\"].astype(np.int64)\n",
    "        smooth_dept_trains.append(smooth_dept_train)\n",
    "\n",
    "    smooth_train = pd.concat(smooth_dept_trains, ignore_index=True)\n",
    "    return smooth_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15cc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(train_file=\"train.csv\", test_file=\"test.csv\", pred_file=\"mypred.csv\", base_folder=\".\", make_post_prediction_adjustment=True):\n",
    "\n",
    "    # Reading train data\n",
    "    train_file_path = f\"{base_folder}/{train_file}\"\n",
    "    test_file_path = f\"{base_folder}/{test_file}\"\n",
    "    pred_file_path = f\"{base_folder}/{pred_file}\"\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    train = pd.read_csv(train_file_path)\n",
    "\n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    test = pd.read_csv(test_file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(\n",
    "        train_pairs, test_pairs, how=\"inner\", on=[\"Store\", \"Dept\"]\n",
    "    )\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr + np.power(Yr, 2) + Wk\",\n",
    "        data=train_split,\n",
    "        return_type=\"dataframe\",\n",
    "    )\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Yr ~ Store + Dept + Yr + np.power(Yr, 2) + Wk\", data=test_split, return_type=\"dataframe\"\n",
    "    )\n",
    "    X[\"Date\"] = test_split[\"Date\"]\n",
    "    test_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train[\"Weekly_Sales\"]\n",
    "        X_train = X_train.drop([\"Weekly_Sales\", \"Store\", \"Dept\"], axis=1)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[[\"Store\", \"Dept\", \"Date\"]]\n",
    "        X_test = X_test.drop([\"Store\", \"Dept\", \"Date\"], axis=1)\n",
    "\n",
    "        tmp_pred[\"Weekly_Pred\"] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    test_pred[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Post-prediction adjustment for fold 5\n",
    "    if make_post_prediction_adjustment:\n",
    "        dates = pd.to_datetime(test_pred[\"Date\"])\n",
    "        test_pred[\"Wk\"] = dates.dt.isocalendar().week\n",
    "\n",
    "        test_pred_51 = test_pred[test_pred[\"Wk\"] == 51]\n",
    "        test_pred_51[\"Shift\"] = test_pred_51[\"Weekly_Pred\"] / 9\n",
    "        test_pred_52 = test_pred[test_pred[\"Wk\"] == 52]\n",
    "\n",
    "        test_pred_52 = test_pred_52.merge(\n",
    "            test_pred_51[[\"Store\", \"Dept\", \"Shift\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        test_pred_51 = test_pred_51.merge(\n",
    "            test_pred_52[[\"Store\", \"Dept\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\", indicator=True\n",
    "        )\n",
    "        test_pred_51[test_pred_51[\"_merge\"] == \"left_only\"][\"Shift\"] = 0\n",
    "\n",
    "        test_pred_52[\"Date\"] = \"2011-12-30\"\n",
    "        test_pred_52[\"Shift\"].fillna(0, inplace=True)\n",
    "        test_pred_52[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "        # test_pred_51[\"Weekly_Pred\"] = test_pred_51[\"Weekly_Pred\"] - test_pred_51[\"Shift\"]\n",
    "        test_pred_52[\"Weekly_Pred\"] = test_pred_52[\"Weekly_Pred\"] + test_pred_52[\"Shift\"]\n",
    "\n",
    "        test_pred_51.drop(\"Shift\", inplace=True, axis=1)\n",
    "        test_pred_52.drop(\"Shift\", inplace=True, axis=1)\n",
    "\n",
    "        test_pred = test_pred[(test_pred[\"Wk\"] != 51) & (test_pred[\"Wk\"] != 52)]\n",
    "        test_pred = pd.concat([test_pred, test_pred_51, test_pred_52], ignore_index=True)\n",
    "        test_pred.drop(columns=[\"Wk\"], inplace=True)\n",
    "\n",
    "    # Save the output to CSV\n",
    "    test_pred[\"Store\"] = test_pred[\"Store\"].astype(np.int64)\n",
    "    test_pred[\"Dept\"] = test_pred[\"Dept\"].astype(np.int64)\n",
    "    \n",
    "    test_pred_final = test.merge(test_pred, on=[\"Store\", \"Dept\", \"Date\"], how=\"left\")\n",
    "    test_pred_final[\"Weekly_Pred\"] = test_pred_final[\"Weekly_Pred\"].fillna(0)\n",
    "    test_pred_final = test_pred_final.loc[:, [\"Store\", \"Dept\", \"Date\", \"IsHoliday\", \"Weekly_Pred\"]]\n",
    "    test_pred_final.to_csv(pred_file_path, index=False)\n",
    "    print(pred_file_path)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a37b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▊                                                                                                          | 1/10 [00:07<01:09,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▌                                                                                              | 2/10 [00:17<01:10,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_2/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [00:25<01:01,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_3/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [00:35<00:55,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████                                                           | 5/10 [00:49<00:53, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 6/10 [01:07<00:52, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████▌                                   | 7/10 [01:20<00:39, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▍                       | 8/10 [01:33<00:26, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9/10 [01:48<00:13, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:04<00:00, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_10/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "\n",
    "for j in tqdm(range(1, num_folds + 1)):\n",
    "\n",
    "    base_folder = f\"Proj2_Data/fold_{j}\"\n",
    "    # Reading train data\n",
    "    train_file = \"train.csv\"\n",
    "    test_file = \"test.csv\"\n",
    "    \n",
    "    process_model(train_file=train_file, test_file=test_file, base_folder=base_folder, make_post_prediction_adjustment=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74362f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold_1=1941.581\n",
      "wae_by_fold_2=1363.493\n",
      "wae_by_fold_3=1382.461\n",
      "wae_by_fold_4=1527.275\n",
      "wae_by_fold_5=2210.984\n",
      "wae_by_fold_6=1635.292\n",
      "wae_by_fold_7=1613.891\n",
      "wae_by_fold_8=1355.014\n",
      "wae_by_fold_9=1336.916\n",
      "wae_by_fold_10=1334.010\n",
      "overall wae=1570.092\n"
     ]
    }
   ],
   "source": [
    "wae = myeval(num_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b4e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
