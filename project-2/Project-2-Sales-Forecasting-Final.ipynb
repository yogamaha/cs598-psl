{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688164c9",
   "metadata": {},
   "source": [
    "### Libraries Allowed\n",
    "* pandas, scipy, numpy\n",
    "* sklearn\n",
    "* datetime\n",
    "* dateutil\n",
    "* prophet\n",
    "* patsy (for model matrix), statsmodels\n",
    "* xgboost\n",
    "* warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62840f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats.mstats import winsorize\n",
    "import datetime\n",
    "import dateutil\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import statsmodels\n",
    "import xgboost\n",
    "import prophet\n",
    "import patsy\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedbb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc57242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval(num_folds=10):\n",
    "    file_path = 'Proj2_Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    #num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'Proj2_Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 10 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    for value in wae:\n",
    "        print(f\"wae_by_fold={value:.3f}\")\n",
    "    print(f\"overall wae={sum(wae) / len(wae):.3f}\")\n",
    "    \n",
    "    # print(wae)\n",
    "    # print(np.mean(wae))    \n",
    "        \n",
    "    return wae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57cebf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "#    data['IsHoliday'] = data['IsHoliday'].apply(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e667b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smooth_train(train):\n",
    "    smooth_dept_trains = []\n",
    "    departments = train['Dept'].unique()\n",
    "\n",
    "    for department in departments:\n",
    "        # Filter rows where Dept is equal to 1\n",
    "        filtered_train = train[train['Dept'] == department]\n",
    "        # Select only the columns 'Store', 'Date', and 'Weekly_Sales'\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        # Pivot table to spread 'Store' values into columns, with 'Weekly_Sales' as values\n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index()\n",
    "\n",
    "        X_train = train_dept_ts.iloc[:, 1:]\n",
    "\n",
    "        # Smooth department data\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_train = np.nan_to_num(X_train)\n",
    "        store_means = np.mean(X_train, axis=0)\n",
    "        X_train = X_train - store_means\n",
    "        X_train = np.transpose(X_train)\n",
    "\n",
    "        U, D, V_t = np.linalg.svd(X_train, full_matrices=False)\n",
    "        D[8:] = 0\n",
    "        F_train = U @ np.diag(D) @ V_t\n",
    "\n",
    "        stores_list = train_dept_ts.columns[1:]\n",
    "        F_train = pd.DataFrame(np.transpose(F_train), columns=stores_list)\n",
    "        F_train = F_train.add(store_means, axis=1)\n",
    "        F_train[\"Date\"] = train_dept_ts[\"Date\"]\n",
    "\n",
    "        smooth_dept_train = pd.melt(F_train, id_vars=['Date'], value_vars = stores_list, \\\n",
    "                                    var_name='Store', value_name='Weekly_Sales')\n",
    "        smooth_dept_train[\"Dept\"] = department\n",
    "        smooth_dept_train[\"Store\"] = smooth_dept_train[\"Store\"].astype(np.int64)\n",
    "        smooth_dept_trains.append(smooth_dept_train)\n",
    "\n",
    "    smooth_train = pd.concat(smooth_dept_trains, ignore_index=True)\n",
    "    return smooth_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a37b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▊                                                                                                          | 1/10 [00:07<01:08,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_1/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▌                                                                                              | 2/10 [00:17<01:09,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_2/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [00:25<01:01,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_3/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [00:36<00:57,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████                                                           | 5/10 [00:49<00:53, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████▊                                               | 6/10 [01:02<00:45, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████████████████████████████▌                                   | 7/10 [01:19<00:40, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████▍                       | 8/10 [01:33<00:27, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9/10 [01:49<00:14, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:03<00:00, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj2_Data/fold_10/mypred.csv\n",
      "CPU times: user 11min 5s, sys: 5min 5s, total: 16min 11s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "for j in tqdm(range(1, num_folds + 1)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Reading train data\n",
    "    file_path = f\"Proj2_Data/fold_{j}/train.csv\"\n",
    "    train = pd.read_csv(file_path)\n",
    "\n",
    "    # Smooth train data\n",
    "    smoothed = pca_smooth_train(train)\n",
    "\n",
    "    train_dupe = train[[\"Date\", \"IsHoliday\"]].drop_duplicates()\n",
    "    train = smoothed.merge(train_dupe, on=[\"Date\"], how=\"left\")\n",
    "\n",
    "    # Reading test data\n",
    "    file_path = f\"Proj2_Data/fold_{j}/test.csv\"\n",
    "    test = pd.read_csv(file_path)\n",
    "\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[[\"Store\", \"Dept\"]].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(\n",
    "        train_pairs, test_pairs, how=\"inner\", on=[\"Store\", \"Dept\"]\n",
    "    )\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr + np.power(Yr, 2)  + Wk\",\n",
    "        data=train_split,\n",
    "        return_type=\"dataframe\",\n",
    "    )\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=[\"Store\", \"Dept\"], how=\"left\")\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices(\n",
    "        \"Yr ~ Store + Dept + Yr + np.power(Yr, 2) + Wk\", data=test_split, return_type=\"dataframe\"\n",
    "    )\n",
    "    X[\"Date\"] = test_split[\"Date\"]\n",
    "    test_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train[\"Weekly_Sales\"]\n",
    "        X_train = X_train.drop([\"Weekly_Sales\", \"Store\", \"Dept\"], axis=1)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "\n",
    "        tmp_pred = X_test[[\"Store\", \"Dept\", \"Date\"]]\n",
    "        X_test = X_test.drop([\"Store\", \"Dept\", \"Date\"], axis=1)\n",
    "\n",
    "        tmp_pred[\"Weekly_Pred\"] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Post-prediction adjustment for fold 5\n",
    "    if j == 5:\n",
    "        dates = pd.to_datetime(test_pred[\"Date\"])\n",
    "        test_pred[\"Wk\"] = dates.dt.isocalendar().week\n",
    "\n",
    "        test_pred_51 = test_pred[test_pred[\"Wk\"] == 51]\n",
    "        test_pred_51[\"Shift\"] = test_pred_51[\"Weekly_Pred\"] / 9\n",
    "        test_pred_52 = test_pred[test_pred[\"Wk\"] == 52]\n",
    "\n",
    "        test_pred_52 = test_pred_52.merge(\n",
    "            test_pred_51[[\"Store\", \"Dept\", \"Shift\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        test_pred_51 = test_pred_51.merge(\n",
    "            test_pred_52[[\"Store\", \"Dept\"]],\n",
    "            on=[\"Store\", \"Dept\"], how=\"left\", indicator=True\n",
    "        )\n",
    "        test_pred_51[test_pred_51[\"_merge\"] == \"left_only\"][\"Shift\"] = 0\n",
    "\n",
    "        test_pred_52[\"Date\"] = \"2011-12-30\"\n",
    "        test_pred_52[\"Shift\"].fillna(0, inplace=True)\n",
    "        test_pred_52[\"Weekly_Pred\"].fillna(0, inplace=True)\n",
    "\n",
    "        # test_pred_51[\"Weekly_Pred\"] = test_pred_51[\"Weekly_Pred\"] - test_pred_51[\"Shift\"]\n",
    "        test_pred_52[\"Weekly_Pred\"] = test_pred_52[\"Weekly_Pred\"] + test_pred_52[\"Shift\"]\n",
    "\n",
    "        test_pred_51.drop(\"Shift\", inplace=True, axis=1)\n",
    "        test_pred_52.drop(\"Shift\", inplace=True, axis=1)\n",
    "\n",
    "        test_pred = test_pred[(test_pred[\"Wk\"] != 51) & (test_pred[\"Wk\"] != 52)]\n",
    "        test_pred = pd.concat([test_pred, test_pred_51, test_pred_52], ignore_index=True)\n",
    "        test_pred.drop(columns=[\"Wk\"], inplace=True)\n",
    "\n",
    "    # Save the output to CSV\n",
    "    file_path = f\"Proj2_Data/fold_{j}/mypred.csv\"\n",
    "    test_pred.to_csv(file_path, index=False)\n",
    "    print(file_path)\n",
    "\n",
    "#     print(\n",
    "#         \"--- fold {fold}, {wae_fold}, {elapsed} seconds ---\".format(\n",
    "#             fold=j,\n",
    "#             wae_fold=f\"{myeval_fold(j, test_with_label):.3f}\",\n",
    "#             elapsed=time.time() - start_time\n",
    "#         )\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73eb0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wae_by_fold=1945.895\n",
      "wae_by_fold=1364.090\n",
      "wae_by_fold=1384.009\n",
      "wae_by_fold=1528.647\n",
      "wae_by_fold=2220.198\n",
      "wae_by_fold=1636.200\n",
      "wae_by_fold=1614.822\n",
      "wae_by_fold=1355.266\n",
      "wae_by_fold=1337.461\n",
      "wae_by_fold=1334.495\n",
      "overall wae=1572.108\n",
      "CPU times: user 803 ms, sys: 502 ms, total: 1.31 s\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wae = myeval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713c399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
