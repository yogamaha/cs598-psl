{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogamaha/cs598-psl/blob/main/Copy_of_PSL_Proj3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "huoPfcwazXZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "HWnACiJnyjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76an1lVLGF-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from google.colab import drive\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso,LassoCV,RidgeCV,RidgeClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import time\n",
        "\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA suggestions:\n",
        "1. use STOP_WORDS by prof\n",
        "1. use both train and test to create vocab\n",
        "2. don't use t-test, just use lasso to reduce vocab to less and 1000\n",
        "3. try tuning model param for GBM\n"
      ],
      "metadata": {
        "id": "XkJk0iYQGZia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOP_WORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "             'you', 'your', 'yours', 'their', 'they', 'his', 'her', 'she',\n",
        "             'he', 'a', 'an', 'and', 'is', 'was', 'are', 'were', 'him',\n",
        "             'himself', 'has', 'have', 'it', 'its', 'the', 'us']\n"
      ],
      "metadata": {
        "id": "Zt5N7S5sMt1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4RwlI6mGPLa",
        "outputId": "64ecaa37-5b0a-4659-c170-04cfcea23391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_path ='/content/drive/My Drive/PSL_data/proj3_data/split_1/train.tsv'\n",
        "test_path = '/content/drive/My Drive/PSL_data/proj3_data/split_1/test.tsv'\n",
        "test_y_path = '/content/drive/My Drive/PSL_data/proj3_data/split_1/test_y.tsv'\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "test_df = pd.read_csv(test_path, sep='\\t')\n",
        "test_y_df = pd.read_csv(test_y_path, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_whole= pd.concat([test_df, test_y_df['sentiment']], axis=1)\n",
        "\n",
        "combined_set = train_df[['review','sentiment']].append(test_whole[['review','sentiment']], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT8cjo5jIAKq",
        "outputId": "9fa89f38-ed1a-476a-92a1-b32d2d7b1b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6c797c933abc>:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_set = train_df[['review','sentiment']].append(test_whole[['review','sentiment']], ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the html tags.\n",
        "combined_set['review'] = combined_set['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)"
      ],
      "metadata": {
        "id": "PdPrD55ENyXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUaYpBT1JyFu"
      },
      "outputs": [],
      "source": [
        "# step 1. Fit count vectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
        "    stop_words=STOP_WORDS,             # Remove stop words\n",
        "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
        "    min_df=0.001,                        # Minimum document frequency\n",
        "    max_df=0.5,                       # Maximum document frequency\n",
        "    token_pattern=r\"\\b[\\w+\\|']+\\b\" # Use word tokenizer\n",
        ")\n",
        "\n",
        "dtm_train = vectorizer.fit_transform(combined_set['review'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtm_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OurIeJVFQ0cy",
        "outputId": "733ce667-9dcd-401c-a290-9212a47cd3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<50000x33011 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 8954742 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Use Lasso for feature selection\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(dtm_train)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.00623)\n",
        "\n",
        "# Fit the model with L1 regularization\n",
        "lasso_model.fit(X_train_scaled, combined_set['sentiment'])\n",
        "\n",
        "# Get the coefficients\n",
        "lasso_coefs = lasso_model.coef_\n",
        "print('Lasso Coefficients:', lasso_coefs.sum())\n",
        "\n",
        "# Create a DataFrame with feature names and their coefficients\n",
        "feature_coef_df = pd.DataFrame(\n",
        "    {'Feature': np.array(vectorizer.get_feature_names_out()),\n",
        "     'Coefficient': lasso_coefs})\n",
        "\n",
        "\"\"\"experiments:\n",
        "alpha=0.001, vocab: ~10k\n",
        "alpha=0.005, vocab: 1440\n",
        "alpha=0.01, vocab: ~400\n",
        "alpha=0.0067, vocab: 863\n",
        "alpha=0.0065, vocab: 925\n",
        "alpha=0.0064, vocab: 942\n",
        "alpha=0.0063, vocab: 973\n",
        "alpha=0.0062, vocab: 1002\n",
        "alpha=0.00625, vocab: 984\n",
        "alpha=0.00623, vocab: 990\n",
        "alpha=0.00621, vocab: 998\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B0w6NcFXA_rk",
        "outputId": "d8ce62d4-e44a-4f05-ed2f-26ec91412c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: -0.22972860162756611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'experiments:\\nalpha=0.001, vocab: ~10k\\nalpha=0.005, vocab: 1440\\nalpha=0.01, vocab: ~400\\nalpha=0.0067, vocab: 863\\nalpha=0.0065, vocab: 925\\nalpha=0.0064, vocab: 942\\nalpha=0.0063, vocab: 973\\nalpha=0.0062, vocab: 1002\\nalpha=0.00625, vocab: 984\\nalpha=0.00623, vocab: 990\\nalpha=0.00621, vocab: 998\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wlk5C3TfUgf",
        "outputId": "9fbdf93d-49c9-41a5-8d7a-92d59f6b0cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "990"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# step 2.1: filter features.\n",
        "selected_features = feature_coef_df[feature_coef_df['Coefficient'] != 0]\n",
        "vocabulary = list(selected_features['Feature'].values)\n",
        "\n",
        "len(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = '/content/drive/My Drive/PSL_data/proj3_data/myvocab.txt'\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/PSL_data/proj3_data/myvocab.txt', 'w') as file:\n",
        "    # Iterate through the list and write each word to a new line\n",
        "    for word in vocabulary:\n",
        "        file.write(word + \"\\n\")\n"
      ],
      "metadata": {
        "id": "maRSlZpZPDmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Open the file and read its contents into a list\n",
        "with open('/content/drive/My Drive/PSL_data/proj3_data/myvocab.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "vocabulary = [line.strip() for line in lines]\n",
        "\n",
        "print (len(vocabulary))"
      ],
      "metadata": {
        "id": "Du4sT8KYjHb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908134c5-a9e3-40c6-899b-93faccd05f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fold_ids = [1,2,3,4,5]\n",
        "\n",
        "train_paths = [\n",
        "    '/content/drive/My Drive/PSL_data/proj3_data/split_{}/train.tsv'.format(i)\n",
        "    for i in fold_ids]\n",
        "test_paths = [\n",
        "    '/content/drive/My Drive/PSL_data/proj3_data/split_{}/test.tsv'.format(i)\n",
        "    for i in fold_ids]\n",
        "\n",
        "test_y_paths = [\n",
        "    '/content/drive/My Drive/PSL_data/proj3_data/split_{}/test_y.tsv'.format(i)\n",
        "    for i in fold_ids]\n",
        "\n",
        "\n",
        "train_dfs = [pd.read_csv(train_path, sep='\\t') for train_path in train_paths]\n",
        "test_dfs = [pd.read_csv(test_path, sep='\\t') for test_path in test_paths]\n",
        "\n",
        "test_y_dfs = [pd.read_csv(testy, sep='\\t') for testy in test_y_paths]"
      ],
      "metadata": {
        "id": "UOtxd0XBqK9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train model - GradientBoostingRegressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    ngram_range=(1, 2)               # Use 1- to 4-grams\n",
        ")\n",
        "vectorizer.fit(vocabulary)\n",
        "\n",
        "\n",
        "for traindf, testdf, testy in zip(train_dfs, test_dfs, test_y_dfs):\n",
        "\n",
        "    model_train = vectorizer.transform(traindf['review'])\n",
        "    model_test = vectorizer.transform(testdf['review'])\n",
        "\n",
        "    tree_regressor = GradientBoostingRegressor(\n",
        "    learning_rate=0.02, n_estimators=1000, subsample=1,max_depth=6)\n",
        "\n",
        "    tree_regressor.fit(model_train, traindf['sentiment'])\n",
        "\n",
        "    predictions = tree_regressor.predict(model_test)\n",
        "\n",
        "    auc_score = roc_auc_score(testy['sentiment'], predictions)\n",
        "\n",
        "    print(f'AUC Score: {auc_score:.3f}')\n"
      ],
      "metadata": {
        "id": "BNNGoBE4i_s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b60a1d-2aaf-4198-bfca-f3b3907f11d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.922\n",
            "AUC Score: 0.918\n",
            "AUC Score: 0.921\n",
            "AUC Score: 0.920\n",
            "AUC Score: 0.920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train model - Ridge regressor\n",
        "vectorizer = CountVectorizer(\n",
        "    ngram_range=(1, 4)               # Use 1- to 4-grams\n",
        ")\n",
        "vectorizer.fit(vocabulary)\n",
        "\n",
        "\n",
        "for traindf, testdf, testy in zip(train_dfs, test_dfs, test_y_dfs):\n",
        "    X_train = vectorizer.transform(traindf['review'])\n",
        "    X_test = vectorizer.transform(testdf['review'])\n",
        "\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    ridge_model = Ridge(alpha = 1)\n",
        "    ridge_model.fit(X_train, traindf['sentiment'])\n",
        "    predictions = ridge_model.predict(X_test)\n",
        "\n",
        "\n",
        "    auc_score = roc_auc_score(testy['sentiment'], predictions)\n",
        "\n",
        "    print(f'AUC Score: {auc_score:.3f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "44rFtU8jYvsp",
        "outputId": "495d07fd-abb8-4681-a268-55d1214fe90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e430b56ca71a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtraindf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Authors: Olivier Grisel <olivier.grisel@ensta.org>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#          Mathieu Blondel <mathieu@mblondel.org>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#          Lars Buitinck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#          Robert Layton <robertlayton@gmail.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#          Jochen Wersd√∂rfer <jochen@wersdoerfer.de>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train model - LogisticRegression\n",
        "vectorizer = CountVectorizer(\n",
        "    ngram_range=(1, 2)               # Use 1- to 4-grams\n",
        ")\n",
        "vectorizer.fit(vocabulary)\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "for j in range(1, num_folds + 1):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_path = f\"/content/drive/My Drive/PSL_data/proj3_data/split_{j}/train.tsv\"\n",
        "    test_path = f\"/content/drive/My Drive/PSL_data/proj3_data/split_{j}/test.tsv\"\n",
        "    test_y_path = f\"/content/drive/My Drive/PSL_data/proj3_data/split_{j}/test_y.tsv\"\n",
        "\n",
        "    traindf = pd.read_csv(train_path, sep='\\t')\n",
        "    testdf = pd.read_csv(test_path, sep='\\t')\n",
        "    testy = pd.read_csv(test_y_path, sep='\\t')\n",
        "\n",
        "    traindf['review'] = traindf['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
        "    testdf['review'] = testdf['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
        "\n",
        "\n",
        "    X_train = vectorizer.transform(traindf['review'])\n",
        "    X_test = vectorizer.transform(testdf['review'])\n",
        "\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    scaler.fit(X_train)\n",
        "\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    clf = LogisticRegression(penalty='elasticnet',solver='saga',random_state=4844,C=1, l1_ratio=0.2).fit(X_train, traindf['sentiment'])\n",
        "    predictions = clf.predict_proba(X_test)\n",
        "\n",
        "    auc_score = roc_auc_score(testy['sentiment'], predictions[:,1])\n",
        "\n",
        "\n",
        "    print(f'AUC Score for split {j}: {auc_score:.3f} | Execution time : {round(time.time() - start_time, 4)} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lX-6mH-8ohG",
        "outputId": "8389868c-cc10-4a23-a02a-160d177f61ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score for split 1: 0.960 | Execution time : 43.6736 seconds\n",
            "AUC Score for split 2: 0.960 | Execution time : 45.2871 seconds\n",
            "AUC Score for split 3: 0.960 | Execution time : 45.2158 seconds\n",
            "AUC Score for split 4: 0.961 | Execution time : 49.9903 seconds\n",
            "AUC Score for split 5: 0.960 | Execution time : 43.46 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}